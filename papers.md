## Paper Collection

- [Perception](papers.md#perception)
- [Decision-Making](papers.md#decision-making)
- [Planning](papers.md#planning)
- [Communication](papers.md#communication)
- [End-to-End](papers.md#end-to-end)
- [Dataset and Simulator](papers.md#dataset-and-simulator)
- [Security](papers.md#security)

### Perception
1. `[TITS 24]` Toward Full-Scene Domain Generalization in Multi-Agent Collaborative Bird's Eye View Segmentation for Connected and Autonomous Driving [[PDF](https://ieeexplore.ieee.org/abstract/document/10779389)]
2. `[ICCV 23]` Spatio-temporal domain awareness for multi-agent collaborative perception [[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Spatio-Temporal_Domain_Awareness_for_Multi-Agent_Collaborative_Perception_ICCV_2023_paper.pdf)] [[Webpage](https://ydk122024.github.io/SCOPE/)]
3. `[arXiv]` V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models [[PDF](https://arxiv.org/pdf/2502.09980)]
4. `[ICCV 23]` HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer [[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf)] [[Code](https://github.com/XHwind/HM-ViT)] ![](https://img.shields.io/github/stars/XHwind/HM-ViT.svg?style=social&label=Star&maxAge=2592000)
5. `[CVPR 23]` Query-Centric Trajectory Prediction [[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.pdf)] [[Code](https://github.com/ZikangZhou/QCNet)] ![](https://img.shields.io/github/stars/ZikangZhou/QCNet.svg?style=social&label=Star&maxAge=2592000)
6. `[ICRA 25]` CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query [[PDF](https://arxiv.org/pdf/2502.19313)]

### Decision-Making

### Planning

### Communication

### End-to-End

### Dataset and Simulator

1. `[CoRL'17]` ![CARLA](https://img.shields.io/badge/-CARLA-blue) CARLA: An Open Urban Driving Simulator [[paper](https://arxiv.org/abs/1711.03938)] [[code](https://github.com/carla-simulator/carla)] [[project](https://carla.org)]
   
2. `[ICCV'21]` ![V2X-Sim](https://img.shields.io/badge/-V2X--Sim-blue) V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving [[paper](https://arxiv.org/abs/2202.08449)] [[code](https://github.com/ai4ce/V2X-Sim)] [[project](https://ai4ce.github.io/V2X-Sim)]
   
3. `[ACCV'22]` ![DOLPHINS](https://img.shields.io/badge/-DOLPHINS-blue) DOLPHINS: Dataset for Collaborative Perception Enabled Harmonious and Interconnected Self-Driving [[paper](https://arxiv.org/abs/2207.07609)] [[code](https://github.com/explosion5/Dolphins)] [[project](https://dolphins-dataset.net)]
   
4. `[ICRA'22]` ![OPV2V](https://img.shields.io/badge/-OPV2V-blue) OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication [[paper](https://arxiv.org/abs/2109.07644)] [[code](https://github.com/DerrickXuNu/OpenCOOD)] [[project](https://mobility-lab.seas.ucla.edu/opv2v)]

5. `[ECCV'22]` ![V2X-ViT](https://img.shields.io/badge/-V2X--ViT-blue) V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer [[paper](https://arxiv.org/abs/2203.10638)] [[code](https://github.com/DerrickXuNu/v2x-vit)] [[project](https://drive.google.com/drive/folders/1r5sPiBEvo8Xby-nMaWUTnJIPK6WhY1B6)]

6. `[CVPR'22]` ![AutoCastSim](https://img.shields.io/badge/-AutoCastSim-blue) COOPERNAUT: End-to-End Driving with Cooperative Perception for Networked Vehicles [[paper](https://ut-austin-rpl.github.io/Coopernaut/)] [[code](https://github.com/UT-Austin-RPL/Coopernaut)] [[project](https://ut-austin-rpl.github.io/Coopernaut/)]

7. `[CVPR'22]` ![DAIR-V2X](https://img.shields.io/badge/-DAIR--V2X-blue) DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection [[paper](https://arxiv.org/abs/2204.05575)] [[code](https://github.com/AIR-THU/DAIR-V2X?tab=readme-ov-file)] [[project](https://thudair.baai.ac.cn/index)]
   
8. `[NeurIPS'22]` ![CoPerception-UAV](https://img.shields.io/badge/-CoPerception--UAV-blue) Where2comm: Communication-Efficient Collaborative Perception via Spatial Confidence Maps [[paper&review](https://openreview.net/forum?id=dLL4KXzKUpS)] [[code](https://github.com/MediaBrain-SJTU/where2comm)] [[project](https://siheng-chen.github.io/dataset/coperception-uav)]

9.  `[NeurIPS'23]` ![IRV2V](https://img.shields.io/badge/-IRV2V-blue) Robust Asynchronous Collaborative 3D Detection via Bird's Eye View Flow [[paper&review](https://openreview.net/forum?id=UHIDdtxmVS)] 

10. `[CVPR'23]` ![CoPerception-UAV+](https://img.shields.io/badge/-CoPerception--UAV%2B-blue) Collaboration Helps Camera Overtake LiDAR in 3D Detection [[paper](https://arxiv.org/abs/2303.13560)] [[code](https://github.com/MediaBrain-SJTU/CoCa3D)] [[project](https://siheng-chen.github.io/dataset/CoPerception+)]

11. `[CVPR'23]` ![OPV2V+](https://img.shields.io/badge/-OPV2V%2B-blue) Collaboration Helps Camera Overtake LiDAR in 3D Detection [[paper](https://arxiv.org/abs/2303.13560)] [[code](https://github.com/MediaBrain-SJTU/CoCa3D)] [[project](https://siheng-chen.github.io/dataset/CoPerception+)]

12. `[CVPR'23]` ![V2V4Real](https://img.shields.io/badge/-V2V4Real-blue) V2V4Real: A Large-Scale Real-World Dataset for Vehicle-to-Vehicle Cooperative Perception [[paper](https://arxiv.org/abs/2303.07601)] [[code](https://github.com/ucla-mobility/V2V4Real)] [[project](https://mobility-lab.seas.ucla.edu/v2v4real)]

13. `[CVPR'23]` ![DAIR-V2X-Seq](https://img.shields.io/badge/-DAIR--V2X--Seq-blue) V2X-Seq: The Large-Scale Sequential Dataset for the Vehicle-Infrastructure Cooperative Perception and Forecasting [[paper](https://arxiv.org/abs/2305.05938)] [[code](https://github.com/AIR-THU/DAIR-V2X-Seq)] [[project](https://thudair.baai.ac.cn/index)]

14. `[ICRA'23]` ![DAIR-V2X-C Complemented](https://img.shields.io/badge/-DAIR--V2X--C-blue) Robust Collaborative 3D Object Detection in Presence of Pose Errors [[paper](https://arxiv.org/abs/2211.07214)] [[code](https://github.com/yifanlu0227/CoAlign)] [[project](https://siheng-chen.github.io/dataset/dair-v2x-c-complemented)]

15. `[ICCV'23]` ![Roadside-Opt](https://img.shields.io/badge/-Roadside--Opt-blue) Optimizing the Placement of Roadside LiDARs for Autonomous Driving [[paper](https://arxiv.org/abs/2310.07247)] 

16. `[AAAI'24]` ![DeepAccident](https://img.shields.io/badge/-DeepAccident-blue) DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving [[paper](https://arxiv.org/abs/2304.01168)] [[code](https://github.com/tianqi-wang1996/DeepAccident)] [[project](https://deepaccident.github.io)]

17. `[ICLR'24]` ![OPV2V-H](https://img.shields.io/badge/-OPV2V--H-blue) An Extensible Framework for Open Heterogeneous Collaborative Perception [[paper&review](https://openreview.net/forum?id=KkrDUGIASk)] [[code](https://github.com/yifanlu0227/HEAL)] [[project](https://huggingface.co/datasets/yifanlu/OPV2V-H)]

18. `[CVPR'24]` ![HoloVIC](https://img.shields.io/badge/-HoloVIC-blue) HoloVIC: Large-Scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative [[paper](https://arxiv.org/abs/2403.02640)]  [[project](https://holovic.net)]

19. `[CVPR'24]` ![Open Mars Dataset](https://img.shields.io/badge/-Open%20Mars%20Dataset-blue) Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset [[paper](https://arxiv.org/abs/2406.09383)] [[code](https://github.com/ai4ce/MARS)] [[project](https://ai4ce.github.io/MARS)]

20. `[CVPR'24]` ![RCooper](https://img.shields.io/badge/-RCooper-blue) RCooper: A Real-World Large-Scale Dataset for Roadside Cooperative Perception [[paper](https://arxiv.org/abs/2403.10145)] [[code](https://github.com/AIR-THU/DAIR-RCooper)] [[project](https://www.t3caic.com/qingzhen)]

21. `[CVPR'24]` ![TUMTraf-V2X](https://img.shields.io/badge/-TUMTraf--V2X-blue) TUMTraf V2X Cooperative Perception Dataset [[paper](https://arxiv.org/abs/2403.01316)] [[code](https://github.com/tum-traffic-dataset/tum-traffic-dataset-dev-kit)] [[project](https://tum-traffic-dataset.github.io/tumtraf-v2x)]

22. `[CVPR'24]` ![ChatSim](https://img.shields.io/badge/-ChatSim-blue) Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents [[paper](https://arxiv.org/abs/2402.05746)] [[code](https://github.com/yifanlu0227/ChatSim)] [[project](https://yifanlu0227.github.io/ChatSim/)]

22. `[ECCV'24]` ![H-V2X](https://img.shields.io/badge/-H--V2X-blue) H-V2X: A Large Scale Highway Dataset for BEV Perception [[paper](https://eccv2024.ecva.net/virtual/2024/poster/126)] 

23. `[NeurIPS'24]` ![DAIR-V2X-Traj](https://img.shields.io/badge/-DAIR--V2X--Traj-blue) Learning Cooperative Trajectory Representations for Motion Forecasting [[paper](https://arxiv.org/abs/2311.00371)] [[code](https://github.com/AIR-THU/V2X-Graph)] [[project](https://thudair.baai.ac.cn/index)]
    
24. `[NeurIPS'24]` ![SMART](https://img.shields.io/badge/-SMART-blue) SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction [[paper](https://arxiv.org/abs/2405.15677)] [[code](https://github.com/rainmaker22/SMART)] [[project](https://smart-motion.github.io/smart/)]


25. `[CVPR'25]` ![Mono3DVLT-V2X](https://img.shields.io/badge/-Mono3DVLT--V2X-blue) Mono3DVLT: Monocular-Video-Based 3D Visual Language Tracking [[paper](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers)]

26. `[CVPR'25]` ![RCP-Bench](https://img.shields.io/badge/-RCP--Bench-blue) RCP-Bench: Benchmarking Robustness for Collaborative Perception Under Diverse Corruptions [paper](https://cvpr.thecvf.com/virtual/2025/poster/34639)] 

27. `[CVPR'25]` ![V2X-R](https://img.shields.io/badge/-V2X--R-blue) V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with Denoising Diffusion [[paper](https://arxiv.org/abs/2411.08402)] [[code](https://github.com/ylwhxht/V2X-R)] 

28. `[arXiv]` ![Adver-City](https://img.shields.io/badge/-Adver--City-blue) Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception Under Adverse Weather Conditions [[paper](https://arxiv.org/abs/2410.06380)] [[code](https://github.com/QUARRG/Adver-City)] [[project](https://labs.cs.queensu.ca/quarrg/datasets/adver-city)]

29. `[arXiv]` ![CP-GuardBench](https://img.shields.io/badge/-CP--GuardBench-blue) CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception [[paper&review](https://openreview.net/forum?id=9MNzHTSDgh)]

30. `[arXiv]` ![DriveGen](https://img.shields.io/badge/-DriveGen-blue) DriveGen: Toward Infinite Diverse Traffic Scenarios with Large Models [[paper](https://arxiv.org/pdf/2503.05808)] 

31. `[arXiv]` ![Griffin](https://img.shields.io/badge/-Griffin-blue) Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark [[paper](https://arxiv.org/abs/2503.06983)] [[code](https://github.com/wang-jh18-SVM/Griffin)] [[project](https://pan.baidu.com/s/1NDgsuHB-QPRiROV73NRU5g)]

32. `[arXiv]` ![InScope](https://img.shields.io/badge/-InScope-blue) InScope: A New Real-world 3D Infrastructure-side Collaborative Perception Dataset for Open Traffic Scenarios [[paper](https://arxiv.org/abs/2407.21581)] [[code](https://github.com/xf-zh/InScope)]

33. `[arXiv]` ![Mixed Signals](https://img.shields.io/badge/-Mixed%20Signals-blue) Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration [[paper](https://arxiv.org/abs/2502.14156)] [[code](https://github.com/chinitaberrio/Mixed-Signals)] [[project](https://mixedsignalsdataset.cs.cornell.edu)]

34. `[arXiv]` ![Multi-V2X](https://img.shields.io/badge/-Multi--V2X-blue) Multi-V2X: A Large Scale Multi-modal Multi-penetration-rate Dataset for Cooperative Perception [[paper](https://arxiv.org/abs/2409.04980)] [[code](https://github.com/RadetzkyLi/Multi-V2X)] 

35. `[arXiv]` ![OPV2V-N](https://img.shields.io/badge/-OPV2V--N-blue) RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling [[paper](https://arxiv.org/abs/2405.16868)] 

36. `[arXiv]` ![V2V-QA](https://img.shields.io/badge/-V2V--QA-blue) V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models [[paper](https://arxiv.org/abs/2502.09980)] [[code](https://github.com/eddyhkchiu/V2VLLM)] [[project](https://eddyhkchiu.github.io/v2vllm.github.io)]

37. `[arXiv]` ![V2XPnP-Seq](https://img.shields.io/badge/-V2XPnP--Seq-blue) V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction [[paper](https://arxiv.org/abs/2412.01812)] [[code](https://github.com/Zewei-Zhou/V2XPnP)] [[project](https://mobility-lab.seas.ucla.edu/v2xpnp)]

38. `[arXiv]` ![V2X-Radar](https://img.shields.io/badge/-V2X--Radar-blue) V2X-Radar: A Multi-Modal Dataset with 4D Radar for Cooperative Perception [[paper](https://arxiv.org/abs/2411.10962)]  [[project](http://openmpd.com/column/V2X-Radar)]

39. `[arXiv]` ![V2X-Real](https://img.shields.io/badge/-V2X--Real-blue) V2X-Real: a Large-Scale Dataset for Vehicle-to-Everything Cooperative Perception [[paper](https://arxiv.org/abs/2403.16034)] [[project](https://mobility-lab.seas.ucla.edu/v2x-real)]

40. `[arXiv]` ![V2X-ReaLO](https://img.shields.io/badge/-V2X--ReaLO-blue) V2X-ReaLO: An Open Online Framework and Dataset for Cooperative Perception in Reality [[paper](https://arxiv.org/abs/2503.10034)] 

41. `[arXiv]` ![WHALES](https://img.shields.io/badge/-WHALES-blue) WHALES: A Multi-Agent Scheduling Dataset for Enhanced Cooperation in Autonomous Driving [[paper](https://arxiv.org/abs/2411.13340)] [[code](https://github.com/chensiweiTHU/WHALES)] [[project](https://pan.baidu.com/s/1dintX-d1T-m2uACqDlAM9A)]

42. `[arXiv]` ![DriveGen-CS](https://img.shields.io/badge/-DriveGen--CS-blue) DriveGen: Towards Infinite Diverse Traffic Scenarios with Large Models [[paper](https://arxiv.org/pdf/2503.05808)] 

43. `[arXiv]` ![V2Xverse](https://img.shields.io/badge/-V2Xverse-blue) Towards Collaborative Autonomous Driving: Simulation Platform and End-to-End System [[paper](https://arxiv.org/abs/2404.09496)] [[code](https://github.com/CollaborativePerception/V2Xverse)] 



### Security

1. `[ICCV'21]` Adversarial Attacks On Multi-Agent Communication [[PDF](https://ieeexplore.ieee.org/document/9711249/?arnumber=9711249)]
2. `[ICCV'23]` Among Us: Adversarially Robust Collaborative Perception by Consensus [[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Among_Us_Adversarially_Robust_Collaborative_Perception_by_Consensus_ICCV_2023_paper.pdf), [Code](https://github.com/coperception/ROBOSAC)] ![](https://img.shields.io/github/stars/coperception/ROBOSAC.svg?style=social&label=Star&maxAge=2592000)
3. `[VehicleSec'23]` Cooperative Perception for Safe Control of Autonomous Vehicles under LiDAR Spoofing Attacks [[PDF](http://arxiv.org/abs/2302.07341)]
4. `[USENIX Security'24]` On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures [[PDF](http://arxiv.org/abs/2309.12955)]
5. `[IROS'24]` Malicious Agent Detection for Robust Multi-Agent Collaborative Perception [[PDF](http://arxiv.org/abs/2310.11901), [Code](https://github.com/shengyin1224/MADE)] ![](https://img.shields.io/github/stars/shengyin1224/MADE.svg?style=social&label=Star&maxAge=2592000)
6. `[AAAI'25]` CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception [[PDF](https://arxiv.org/abs/2412.12000)]
7. `[AAAI'24]` Robust Communicative Multi-Agent Reinforcement Learning with Active Defense [[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/29708)]
8. `[arXiv]` GCP: Guarded Collaborative Perception with Spatial-Temporal Aware Malicious Agent Detection [[PDF](https://arxiv.org/abs/2501.02450)]
9. `[arXiv]` CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception [[PDF](https://arxiv.org/abs/2502.07807v1)]

